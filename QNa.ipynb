{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### qNa\n",
    "\n",
    "Explain Focal loss ?\n",
    "\n",
    "- Focal loss is a modification of the cross-entropy loss function for use in binary classification problems where there is a significant class imbalance.\n",
    "\n",
    "> The basic idea behind focal loss is to reduce the weight assigned to well-classified examples and to increase the weight assigned to misclassified examples, with the hope of improving the training of a model in the presence of a large number of easy examples\n",
    "\n",
    "The focal loss function is defined as:\n",
    "\n",
    "`FL(p_t) = -α_t * (1 - p_t)^γ * log(p_t)`\n",
    "\n",
    "-   p_t is the predicted probability of the correct class\n",
    "-   α_t is a weighting factor that depends on the true class label (i.e., 0 or 1)\n",
    "-   γ is a tunable focusing parameter that controls` the rate at which the weight assigned to well-classified examples is reduced`.\n",
    "\n",
    "Advantages of Focal Loss:\n",
    "\n",
    "-   Focal Loss can help address the class imbalance problem that is often present in classification tasks, where one class may have significantly fewer examples than others. The Focal Loss function assigns more weight to hard-to-classify examples, which can help improve the model's accuracy on the minority class.\n",
    "-   Focal Loss can help reduce the impact of easy-to-classify examples that contribute little to the training of the model.\n",
    "\n",
    "`Focal Loss can be used in this scenario to give more emphasis to the rare class, and down-weight the easy examples. This way, the model can be trained to better capture the patterns in the rare class and improve the overall performance of the model.`\n",
    "\n",
    "\n",
    "If you are attempting to predict a customer's gender with only 100 data points, there are several problems that could arise due to the small sample size:\n",
    "\n",
    "1.  **Sampling Bias**: The sample may not be representative of the population, leading to a biased dataset. For example, the data might be skewed towards a specific geographic location or age group, which could lead to incorrect predictions for the larger population.\n",
    "    \n",
    "2.  **Overfitting**: With a small dataset, the model may learn the noise in the data rather than the underlying patterns, resulting in overfitting. This means the model may perform well on the training data but poorly on new, unseen data.\n",
    "    \n",
    "3.  **Underfitting**: On the other hand, if the model is too simple, it may underfit the data, resulting in poor predictions.\n",
    "    \n",
    "4.  L**ack of statistical power**: A small sample size may not have enough statistical power to detect meaningful differences or relationships, leading to unreliable results.\n",
    "    \n",
    "5.  **Unbalanced classes:** A small sample size may also result in imbalanced classes, where one class is overrepresented and the other is underrepresented. This can lead to biased predictions towards the overrepresented class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Matrix factorization is a technique in linear algebra that involves breaking down a given matrix into a product of two or more smaller matrices.`\n",
    "\n",
    "> The resulting factors can be used to extract useful information from the original matrix, such as hidden patterns or relationships between variables.\n",
    "\n",
    "---\n",
    "\n",
    "> PCs (Principal Components) is a technique used to analyze and reduce the dimensionality of data. It involves finding the linear combinations of variables that explain the maximum amount of variance in the data.\n",
    "While both PCs and SVD are used for dimensionality reduction and feature extraction, they have some important differences.\n",
    "\n",
    "> SVD (Singular Value Decomposition) is a matrix factorization technique that decomposes a matrix into three matrices: a left singular matrix, a diagonal matrix of singular values, and a right singular matrix\n",
    "\n",
    "\n",
    "-   PCs are obtained through the `eigendecomposition of the covariance matrix`, while SVD is a matrix factorization technique.\n",
    "-   PCs are a linear combination of the original variables, while SVD provides a factorization of the matrix itself.\n",
    "-   PCs are often used in exploratory data analysis and visualization, while SVD is used in a wide range of applications, including image compression, recommendation systems, and collaborative filtering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trnasformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.  What are the steps involved in the self-attention mechanism?\n",
    "\n",
    "The steps involved in the self-attention mechanism are as follows:\n",
    "\n",
    "-   **Input embeddings:** The input sentence is first embedded into a vector space using an embedding layer. Each word in the sentence is represented as a vector of fixed dimension.\n",
    "    \n",
    "-   **Query, key, and value matrices:** For each word in the sentence, the self-attention mechanism creates three vectors - the query vector, key vector, and value vector. These vectors are used to compute the attention weights for that word.\n",
    "    \n",
    "-   **Attention weights**: The attention weights are calculated by taking the dot product of the query vector with the key vector and then applying a softmax function to the result. This produces a set of weights that determine how much attention should be given to each word in the sentence.\n",
    "    \n",
    "-   **Weighted sum:** Once the attention weights are calculated, the value vectors for each word are multiplied by their corresponding attention weights and then summed together. This produces a weighted sum of the value vectors, which represents the context vector for that word.\n",
    "    \n",
    "-   **Multi-head attention:** To improve the effectiveness of the attention mechanism, the self-attention mechanism can be extended to use multiple sets of query, key, and value matrices. These sets are called heads, and the attention weights and context vectors from each head are concatenated together before being passed to the next layer.\n",
    "    \n",
    "\n",
    "2.  What is scaled dot product attention?\n",
    "\n",
    "Scaled dot product attention is a type of attention mechanism used in the Transformer architecture. **It is used to compute the attention weights between the query and key vectors.**  The steps involved in scaled dot product attention are as follows:\n",
    "\n",
    "-   Compute the dot product of the query and key vectors.\n",
    "-   Scale the dot product by the square root of the dimension of the key vectors. This is done to prevent the dot product from becoming too large, which can cause numerical instability when applying the softmax function.\n",
    "-   Apply a softmax function to the scaled dot product to obtain the attention weights.\n",
    "-   Multiply the attention weights by the value vectors to obtain the weighted sum, which represents the context vector.\n",
    "\n",
    "3.  How do we create the query, key, and value matrices?\n",
    "\n",
    "To create the query, key, and value matrices for the self-attention mechanism, **we first apply an embedding layer to the input sentence**. This converts each word in the sentence to a vector of fixed dimension. **These vectors are then used to create the query, key, and value matrices as follows:**\n",
    "\n",
    "-   For each word in the sentence, we create a query vector, a key vector, and a value vector. These vectors are obtained by multiplying the embedding of the word by three matrices - the query matrix, the key matrix, and the value matrix. These matrices are learned during training.\n",
    "-   The query, key, and value matrices are then stacked together to create the input for the attention mechanism. Specifically, the query matrix is used to compute the attention weights, the key matrix is used to provide the context for computing the attention weights, and the value matrix is used to create the weighted sum that represents the context vector.\n",
    "\n",
    "4.  Why do we need positional encoding?\n",
    "\n",
    "The self-attention mechanism in the Transformer architecture does not take into account the order of the words in the sentence. To address this issue, the Transformer architecture uses positional encoding.\n",
    "\n",
    "- Positional encoding is a technique used to encode the position of each word in the sentence. This is done by adding a fixed-length vector to the embedding of each word.\n",
    " - The vector contains information about the position of the word in the sentence, and is learned during training. By adding this vector to the embedding of each word,\n",
    "\n",
    "5.  What are the sublayers of the decoder?\n",
    "\n",
    "The decoder in the Transformer architecture is responsible for generating the output sequence. It consists of several sublayers, which are applied in sequence to the input. The sublayers of the decoder are:\n",
    "\n",
    "-   **Masked multi-head self-attention**: This sublayer is similar to the self-attention mechanism used in the encoder, but with a mask applied to prevent each position from attending to subsequent positions. This is done to ensure that the decoder only attends to positions that have already been generated in the output sequence.\n",
    "    \n",
    "-   **Multi-head encoder-decoder attention**: This sublayer is used to attend to the encoder output. It takes the decoder input and the encoder output as inputs, and produces a context vector that summarizes the important information from the encoder for generating the next output.\n",
    "    \n",
    "-   **Feedforward network**: This sublayer is a simple neural network with two linear layers and a ReLU activation in between. It is applied to the output of the attention sublayers to produce the final output.\n",
    "    \n",
    "\n",
    "6.  What are the inputs to the encoder-decoder attention layer of the decoder?\n",
    "\n",
    "The encoder-decoder attention layer of the decoder takes two inputs: \n",
    "\t1 . the decoder input and \n",
    "\t2. the encoder output. \n",
    "The decoder input is a sequence of embeddings representing the target sentence. \n",
    "The encoder output is a sequence of embeddings representing the source sentence that has been processed by the encoder.\n",
    "\n",
    " `The encoder-decoder attention layer calculates the attention weights between the decoder input and the encoder output, and produces a context vector that summarizes the important information from the encoder for generating the next output in the target sequence.`\n",
    " \n",
    " `This context vector is then used as input to the next sublayer in the decoder.`\n",
    "\n",
    "7.  What is the difference between self-attention and convolutional neural networks?\n",
    "\n",
    "Self-attention and convolutional neural networks (CNNs) are both used in natural language processing and computer vision tasks. However, there are some differences between the two.\n",
    "\n",
    "In CNNs, `a small kernel is used to slide over the input and produce features`. This kernel is the same for every position in the input, `which means that the network is not able to capture long-range dependencies between different parts of the input`.\n",
    "\n",
    "On the other hand, self-attention is able to capture` long-range dependencies because it allows each position in the input to attend to all other positions`. This means that self-attention can learn relationships between any two positions in the input, regardless of their distance from each other.\n",
    "\n",
    "10.  What is the significance of the term \"Transformer\" in the Transformer architecture?\n",
    "\n",
    "The term \"Transformer\" refers to the fact that the `model uses only self-attention and feedforward layers`, `which are \"transformer\" layers that do not depend on the order of the input sequence`. This is in contrast to other models like recurrent neural networks, which use recurrent layers that depend on the order of the input sequence.\n",
    "\n",
    "The use of transformer layers allows the model to capture long-range dependencies between different parts of the input sequence, which is important for tasks like language translation where word order is important. Additionally, the use of transformer layers `makes the model more parallelizable and efficient, since each token in the input sequence can be processed independently.`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
