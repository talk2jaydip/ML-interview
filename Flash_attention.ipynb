{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## 1. Introduction: What Is Flash Attention?\n",
    "\n",
    "**Flash Attention** is an optimized, IO-aware exact attention mechanism designed for transformer models—especially large language models (LLMs) like GPT, BERT, and others. Research indicates that it significantly improves both speed and memory efficiency. Key benefits include:\n",
    "\n",
    "- **Speed Improvements:** Benchmarks have shown speedups of 2–4× in training and inference. For example, BERT-large training can be around 15% faster, and GPT2 training may be accelerated up to three times.\n",
    "- **Memory Efficiency:** Standard attention scales quadratically with sequence length (\\(O(N^2)\\)); Flash Attention reduces this to linear scaling (\\(O(N)\\)) by using a technique called tiling.\n",
    "- **Exactness:** Unlike some approximate methods, Flash Attention produces the same results as standard attention, making it a true drop-in replacement.\n",
    "- **Applicability:** It’s especially useful for tasks involving long sequences (e.g., language translation, chatbots) and is integrated into frameworks like Hugging Face’s Transformers and implemented via libraries like Triton.\n",
    "\n",
    "*Key research and implementations have even extended Flash Attention to block-sparse variants, enabling efficient processing of sequences up to 64k tokens.*  \n",
    "citeturn0search0  \n",
    "citeturn0search7\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Standard Attention vs. Flash Attention\n",
    "\n",
    "### Standard Attention\n",
    "\n",
    "In the seminal transformer paper (\"Attention Is All You Need\"), the attention mechanism is computed as follows:\n",
    "\n",
    "$$\n",
    "\\text{Attn}(Q, K, V) = \\text{Softmax}\\left(\\frac{QK^T}{\\sqrt{d}}\\right)V\n",
    "$$\n",
    "\n",
    "- **Input Projections:**  \n",
    "  The input embeddings are projected into three matrices: Query (\\(Q\\)), Key (\\(K\\)), and Value (\\(V\\)).\n",
    "- **Computation:**  \n",
    "  The product \\(QK^T\\) generates an \\(N \\times N\\) matrix (with \\(N\\) being the sequence length), and applying softmax row-wise yields the attention probabilities.\n",
    "- **Complexity Issue:**  \n",
    "  The full attention matrix requires \\(O(N^2)\\) memory and many data transfers between high-bandwidth memory (HBM) and fast on-chip SRAM, which makes it impractical for long sequences.\n",
    "\n",
    "### Flash Attention\n",
    "\n",
    "Flash Attention addresses these limitations through an IO-aware design that minimizes unnecessary memory transfers and leverages the GPU’s memory hierarchy.\n",
    "\n",
    "- **Tiling:**  \n",
    "  Instead of computing and storing the entire \\(N \\times N\\) attention matrix, the algorithm partitions the Q, K, and V matrices into smaller blocks (tiles) that fit into fast SRAM.\n",
    "  \n",
    "  - **Result:**  \n",
    "    Memory complexity drops from \\(O(N^2)\\) to \\(O(N)\\), as each tile is processed independently.\n",
    "  \n",
    "- **Partial Computation and Accumulation:**  \n",
    "  For each tile:\n",
    "  - **Dot Product:**  \n",
    "    Compute partial scores:  \n",
    "    $$\n",
    "    S_{i,j} = \\frac{Q_i K_j^T}{\\sqrt{d}}\n",
    "    $$\n",
    "  - **Local Softmax:**  \n",
    "    Instead of a single softmax across the whole row, the softmax is computed per tile. Local statistics such as the maximum value (\\(m_i\\)) and sum of exponentials (\\(l_i\\)) are maintained.\n",
    "  - **Re-normalization:**  \n",
    "    As new tiles are processed, these statistics are updated using formulas like:\n",
    "    $$\n",
    "    m_{\\text{new}, i} = \\max(m_i, m_{\\tilde{i},j})\n",
    "    $$\n",
    "    $$\n",
    "    l_{\\text{new}, i} = l_i \\cdot e^{m_i - m_{\\text{new}, i}} + l_{\\tilde{i},j} \\cdot e^{m_{\\tilde{i},j} - m_{\\text{new}, i}}\n",
    "    $$\n",
    "    This ensures that the final softmax output is exactly the same as if computed in one go.\n",
    "  - **Output Accumulation:**  \n",
    "    The weighted output is accumulated for each block:\n",
    "    $$\n",
    "    O_i = \\text{diag}(l_i) \\cdot \\left( O_i \\cdot e^{m_i - m_{\\text{new}, i}} \\right) + P_{\\tilde{i},j} \\cdot V_j \\cdot e^{m_{\\tilde{i},j} - m_{\\text{new}, i}}\n",
    "    $$\n",
    "    where \\(P_{\\tilde{i},j}\\) represents the normalized partial attention weights.\n",
    "    \n",
    "- **IO-Aware Execution:**  \n",
    "  By fusing the operations (dot product, softmax, and weighted sum) into a single GPU kernel, Flash Attention minimizes slow HBM-to-SRAM transfers, significantly boosting speed.\n",
    "\n",
    "*This design not only accelerates the forward pass but also efficiently recomputes intermediates during backpropagation without storing the full \\(N \\times N\\) matrix.*  \n",
    "citeturn0search4  \n",
    "citeturn0search1\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Detailed Comparison\n",
    "\n",
    "| **Aspect**                | **Standard Attention**                                   | **Flash Attention**                                                 |\n",
    "|---------------------------|----------------------------------------------------------|----------------------------------------------------------------------|\n",
    "| **Memory Usage**          | Requires full \\(N \\times N\\) matrix (\\(O(N^2)\\))         | Processes tiles; memory scales linearly (\\(O(N)\\))                    |\n",
    "| **Memory Transfers (IO)** | Frequent transfers between HBM and SRAM                | Fused tiled operations minimize transfers                           |\n",
    "| **Computation Speed**     | Slower due to memory-bound softmax and matrix materialization | 2–4× faster due to reduced data movement and efficient kernel fusion  |\n",
    "| **Numerical Accuracy**    | Standard softmax with numerical tricks                 | Maintains exact results using re-normalization and log-sum-exp techniques |\n",
    "| **Backward Pass**         | Stores full attention for gradient computation         | Recomputes intermediates via saved summary statistics                  |\n",
    "\n",
    "*Key takeaway:* Flash Attention is especially beneficial for long sequences, reducing both computational time and memory requirements, and is ideal for training large models or real-time inference applications.\n",
    "\n",
    "citeturn0search5\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Usage and Implementation\n",
    "\n",
    "### When and Where to Use Flash Attention\n",
    "- **Large Models & Long Sequences:**  \n",
    "  Essential for training LLMs or running inference on tasks that involve very long contexts.\n",
    "- **Real-Time Applications:**  \n",
    "  Reduces latency in applications like chatbots or translation services.\n",
    "- **Supported Frameworks:**  \n",
    "  Already integrated into some libraries (e.g., Hugging Face Transformers with a “flash” prefix) and implemented through optimized CUDA kernels or frameworks like Triton.\n",
    "\n",
    "### Hardware and Software Requirements\n",
    "- **Modern GPUs:**  \n",
    "  Requires GPUs with ample HBM and high-speed SRAM (e.g., NVIDIA A100, Hopper series). Older GPUs like the V100 may not fully benefit from these optimizations.\n",
    "- **Software Tools:**  \n",
    "  Custom CUDA kernels or higher-level libraries like Triton facilitate the development and integration of Flash Attention into deep learning pipelines.\n",
    "\n",
    "### Implementation Challenges\n",
    "- **Kernel Optimization:**  \n",
    "  Writing and tuning CUDA kernels for efficient tile-based computation can be challenging. Triton helps abstract some of this complexity.\n",
    "- **Numerical Stability:**  \n",
    "  Although Flash Attention maintains exactness, careful implementation (such as using BF16) is required to manage potential numerical deviations.\n",
    "- **Compatibility:**  \n",
    "  The method may require adaptations for different GPU architectures.\n",
    "\n",
    "*Real-world performance improvements include up to 70% of the theoretical max FLOPS and practical speedups on models like BERT-large and GPT2.*  \n",
    "citeturn0search3\n",
    "\n",
    "---\n",
    "\n",
    "## 5. How Flash Attention Works: A Step-by-Step Outline\n",
    "\n",
    "1. **Tiling the Input:**\n",
    "   - Divide the Q, K, and V matrices into smaller blocks based on the capacity of SRAM.\n",
    "   - For example, for a sequence length \\(N = 1000\\) and head dimension \\(d\\), compute appropriate block sizes \\(B_r\\) and \\(B_c\\) to fit the fast memory.\n",
    "\n",
    "2. **Partial Computations:**\n",
    "   - For each tile, compute the scaled dot product:\n",
    "     $$\n",
    "     S_{i,j} = \\frac{Q_i K_j^T}{\\sqrt{d}}\n",
    "     $$\n",
    "   - Calculate local softmax statistics: determine the maximum value and sum of exponentials for each row.\n",
    "\n",
    "3. **Re-normalization and Accumulation:**\n",
    "   - As each tile is processed, update cumulative statistics:\n",
    "     $$\n",
    "     m_{\\text{new}, i} = \\max(m_i, m_{\\tilde{i},j})\n",
    "     $$\n",
    "     $$\n",
    "     l_{\\text{new}, i} = l_i \\cdot e^{m_i - m_{\\text{new}, i}} + l_{\\tilde{i},j} \\cdot e^{m_{\\tilde{i},j} - m_{\\text{new}, i}}\n",
    "     $$\n",
    "   - Accumulate the output using:\n",
    "     $$\n",
    "     O_i = \\text{diag}(l_i) \\cdot \\left( O_i \\cdot e^{m_i - m_{\\text{new}, i}} \\right) + P_{\\tilde{i},j} \\cdot V_j \\cdot e^{m_{\\tilde{i},j} - m_{\\text{new}, i}}\n",
    "     $$\n",
    "   - This process ensures that each tile’s contribution is correctly normalized and added to the final output, guaranteeing numerical stability.\n",
    "\n",
    "4. **Backward Pass:**\n",
    "   - Instead of storing the entire \\(N \\times N\\) matrix, summary statistics are stored. These are later used to recompute gradients efficiently during backpropagation.\n",
    "\n",
    "*This step-by-step strategy leverages the GPU’s memory hierarchy by keeping computations in the fast SRAM as much as possible, thereby reducing overall memory access time.*\n",
    "\n",
    "citeturn0search7\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Conclusion\n",
    "\n",
    "Flash Attention is a breakthrough that transforms the attention mechanism in transformer models by:\n",
    "- **Reducing Memory Footprint:**  \n",
    "  It cuts memory requirements from quadratic to linear, making it feasible to handle longer sequences.\n",
    "- **Enhancing Speed:**  \n",
    "  By fusing tiled operations and reducing memory transfers, it achieves 2–4× faster performance compared to standard attention.\n",
    "- **Maintaining Exactness:**  \n",
    "  Despite all optimizations, it computes the exact same output as the standard method, ensuring reliability.\n",
    "\n",
    "For practitioners, integrating Flash Attention means faster training, quicker inference, and the ability to scale models to handle extensive contexts—vital for state-of-the-art LLMs and real-time applications.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
