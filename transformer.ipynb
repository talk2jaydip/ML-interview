{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to the transformer\n",
    "\n",
    "RNN and LSTM networks Works for  Sequential tasks\n",
    "\n",
    " 1. -  next word prediction\n",
    "   \t- text generation\n",
    "   \t- machine translation\n",
    "   \t- \n",
    " 2. However one of the major challenges with the recurrent `model is capturing the long-term dependency`.\n",
    " 3. \n",
    "`The transformer model uses a special type of attention mechanism called  **self-attention**.`instead of recurrence.\n",
    "\n",
    "1. The transformer consists of an encoder-decoder architecture.\n",
    "2. We feed the input sentence (source sentence) to the encoder.  \n",
    "3. The encoder learns the representation of the input sentence and sends the representation to the decoder. \n",
    "4.  The decoder receives the representation learned by the encoder as input and generates the output sentence (target sentence)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding the encoder of the transformer\n",
    "\n",
    "- The transformer consists of a stack of  `N` number of encoders. \n",
    "- The output of one encoder is sent as input to the encoder above it.\n",
    "- The final encoder returns the representation of the given source sentence as output.\n",
    "- We feed the source sentence as input to the encoder and get the representation of the source sentence as output:\n",
    "\n",
    "![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781838821593/files/assets/bf686ce1-31f7-47b0-a51a-785ba9a756e4.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Encoder has 2 sub layers\n",
    "-   Multi-head attention\n",
    "-   Feedforward network\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Self-attention mechanism\n",
    "\n",
    "`Suppose our input sentence (source sentence) is  _I am good._` First,  \n",
    "1. we get the embeddings for each word in our sentence.  `Note that the embeddings are just the vector representation of the word and the values of the embeddings will be learned during training.`\n",
    "\n",
    "Then, we can represent our input sentence _I am good_ using the input matrix   (embedding matrix or input embedding) as  shown here:\n",
    "![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781838821593/files/assets/c3132147-b254-4212-bf73-d5c7d3e9c3fa.png)\n",
    "\n",
    "Dimension will be : `[_sentence length_ x _embedding dimension_]` in this case \n",
    "`[3 X 12]`\n",
    "\n",
    "- Next from the input matrix `x` we create three new matrices:\n",
    "\t- Query\n",
    "\t- key\n",
    "\t- value\n",
    "\n",
    "Note that the weight matrices Wq,Wk, Wv are `randomly initialized and their optimal values will be learned during training`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781838821593/files/assets/816dafcc-dc33-4166-a0ce-76d8dfba5786.png)\n",
    "\n",
    "- The first row in the query, key, and value matrices q1,k1,v1  and – implies the query, key, and value vectors of the word `I`. \n",
    "- The second row in the query, key, and value matrices q2,k2,v2 and  – implies the query, key, and value vectors of the word `am.` \n",
    "- The third row in the query, key, and value matrices q3,k3,v3 and  – implies the query, key, and value vectors of the word `good`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### how the query, key, and value matrices are used in the self-attention mechanism.\n",
    "\n",
    " - in order to compute a representation of a word, `the self-attention mechanism relates the word to all the words in the given sentence`.\n",
    " - Understanding how a word is related to all the words in the sentence helps us to learn better representation.\n",
    "\n",
    "###  The self-attention mechanism includes four steps; let's take a look at them one by one.\n",
    "\n",
    "in summary:\n",
    "\n",
    "   1. First, we compute the dot product between the query matrix and the\n",
    "    key matrix, , and get the similarity scores. \n",
    "   2.  Next, we divide  by thesquare root of the dimension of the key vector,.\n",
    "   3. Then, we apply the softmax function to normalize the scores and obtain the score\n",
    "    matrix.\n",
    "   4.  At the end, we compute the attention matrix, , bymultiplying the score matrix by the value matrix, .\n",
    "![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781838821593/files/assets/2fecf5e4-32ba-4687-b9af-18df94aa695d.png)\n",
    "\n",
    "`The self-attention mechanism is also called  **scaled dot product attention**,`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781838821593/files/assets/f0ca2ff8-f9bd-4e26-9b39-1f0b66973d8b.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The first step in the self-attention mechanism is to compute the dot product between the query matrix , Key matrix\n",
    "`Computing the dot product between two vectors tells us how similar they are.\n",
    "it give us simmliaruty score to understand how each words are similliar to each other`\n",
    "\n",
    "![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781838821593/files/assets/f0ca2ff8-f9bd-4e26-9b39-1f0b66973d8b.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    " ##### **Step 2**\n",
    "\n",
    "The next step in the self-attention mechanism is to divide the matrix by the square root of the dimension of the key vector.\n",
    "`This is useful in obtaining stable gradients.`\n",
    "\n",
    "![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781838821593/files/assets/e15fbb4d-5083-476f-b27f-76d299961545.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    " #### **Step 3**\n",
    " - Score Matrix\n",
    " `we normalize them using the softmax function. Applying softmax function helps in bringing the score to the range of 0 to 1 and the sum of the scores equals to 1,`\n",
    "![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781838821593/files/assets/6ef2abf9-c9ab-4b33-9dd4-0031cee677a0.png)\n",
    "\n",
    "`can understand how each word in the sentence is related to all the words in the sentence. F`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### **Step 4**\n",
    " > Attention Matirx\n",
    "\t\t\n",
    "\t  It is computed by taking the `sum of the value vectors weighted by the scores`.\n",
    "\n",
    "- The final step in the self-attention mechanism is to compute the attention matrix, .\n",
    "`The attention matrix contains the attention values for each word in the sentence.\n",
    "\n",
    "![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781838821593/files/assets/5f3d7bd4-2854-4cd9-a875-c694d8bd52f0.png)`\n",
    "\n",
    "the self-attention  of the word _I_  is computed:\n",
    "\n",
    "![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781838821593/files/assets/86e1dbc8-dc75-4f28-9683-d70b20bf22e6.png)\n",
    "\n",
    "> Thus, the value of  will contain 90% of the values from the value vector  (I), 7% of the values from the value vector  (am), and 3% of values from the value vector  (good).\n",
    "\n",
    "![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781838821593/files/assets/e5c0e9a1-7930-4ddc-aa28-992822049183.png)\n",
    "\n",
    "- we can understand that the self-attention value of the word `it` contains 100% of the values from the value `vector  (dog)`. \n",
    "- This helps the model to understand that the word it actually `efers to dog and not food`.\n",
    "\n",
    "` Thus, by using a self-attention mechanism, we can understand how a word is related to all other words in the sentence. `\n",
    "\n",
    "Attention is all you need\n",
    "\n",
    "![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781838821593/files/assets/8bf9914c-081f-409f-91da-9c4fc8af0aa3.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### <div style=\"background-color: #d9edf7; border: 1px solid #bce8f1; color: #31708f; padding: 10px; border-radius: 5px; margin: 10px 0;\">\n",
    "#### \n",
    "# Info: Background: #d9edf7, Border: #bce8f1, Text: #31708f\n",
    "# Warning: Background: #fcf8e3, Border: #faebcc, Text: #8a6d3b\n",
    "# Success: Background: #dff0d8, Border: #d0e9c6, Text: #3c763d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Multi-head attention mechanism\n",
    "<div style=\"background-color: #fcf8e3; border: 1px solid #bce8f1; color: #31708f; padding: 10px; border-radius: 5px; margin: 10px 0;\">\n",
    "For better representation to ambiguous word and remove the dominance affect of specific word on other words in sentences .\n",
    "</div>\n",
    "\n",
    "<div style=\"background-color: #d9edf7; border: 1px solid #bce8f1; color: #31708f; padding: 10px; border-radius: 5px; margin: 10px 0;\">\n",
    "\n",
    "The multi-head attention layer is an extension of the self-attention layer. It splits the queries, keys, and values into multiple heads, each with a smaller dimension, and performs self-attention on each head separately. Then it concatenates the outputs of all heads and applies another linear projection. <b>This allows the model to attend to different aspects or features of the input sequence simultaneously.\n",
    "    </b></div>\n",
    "\n",
    "In order to maintain correct representation  `instead of computing a single attention matrix, we will compute multiple attention matrices and then concatenate their results`. \n",
    "\n",
    "> The idea behind using multi-head attention is that instead of using a single attention head, if we use multiple attention heads, then our attention matrix will be more accurate.\n",
    "\n",
    "Suppose we have eight attention matrices, 21 to z8  to ; then, we can just concatenate all the attention heads (attention matrices) and multiply the result by a new weight matrix, Wo , and create the final attention matrix as shown: \n",
    "\n",
    "![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781838821593/files/assets/15ea0152-cd90-4ffd-832a-09e5b931e185.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### positional encoding\n",
    "\n",
    "> the problem is since we feed the words parallel to the transformer,  to maintain the position of words.\n",
    "\n",
    "> So, instead of feeding the input matrix directly to the transformer, we need to add some information indicating the word order (position of the word) so that our network can understand the meaning of the sentence.\n",
    "\n",
    "`The dimension of the positional encoding matrix is same as input matrix`\n",
    "\n",
    "![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781838821593/files/assets/ae0bba23-b709-4214-ab1e-37bef2fedb80.png)\n",
    "\n",
    "> So, now our input matrix will have  not only the embedding of the word but also the position of the word in the sentence:\n",
    "\n",
    "> sinusoidal function for computing the positional encoding, as shown:\n",
    " \n",
    "![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781838821593/files/assets/21aa0af2-6bb1-452a-8de7-2db89751d434.png)\n",
    "\n",
    "\n",
    "![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781838821593/files/assets/e7919233-30fe-4f57-b640-0de5b6f3ea48.png)\n",
    "\n",
    "\n",
    "![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781838821593/files/assets/96739439-541e-45b1-95c0-2052fd761024.png)\n",
    "\n",
    "`Thus, our final positional encoding matrix P`\n",
    "\n",
    "![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781838821593/files/assets/d0600945-874a-400f-8a2c-7c54c9eec48a.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Encoder Block:\n",
    "\n",
    "![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781838821593/files/assets/e11862b0-33b2-4ff6-8ca8-e4bba4777beb.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Feedforward network\n",
    "\n",
    "- The feedforward network consists of `two dense layers with ReLU activations`. - `The parameters of the feedforward network` **are the same over the different positions** of the sentence and **different over the encoder blocks**\n",
    "\n",
    "`The feedforward network takes the attention matrix as input and returns the encoder representation as output.`\n",
    "\n",
    "#### Encoder block with the add and norm component\n",
    "\n",
    "\tIt connects the input and output of a sublayer\n",
    "\n",
    "### Residual Connection\n",
    "The add and norm component is basically a residual connection followed by  **layer normalization**. \n",
    "Layer normalization promotes `faster training by preventing the values in each layer from changing heavily.`\n",
    "\n",
    "![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781838821593/files/assets/6fb3dfd2-7e3f-43fd-baac-0a7e846a41d9.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Final Encoder Block\n",
    "\n",
    "From the preceding figure, we can understand the following:\n",
    "\n",
    "1.  First, we convert our input to an` input embedding (embedding matrix), and then add the position encoding to it and feed it as input to the bottom-most encoder (encoder 1).`\n",
    "2.  Encoder 1 takes the input and sends it to the multi-head attention sublayer, `which returns the attention matrix as output.`\n",
    "\n",
    "3.  We take the attention matrix and feed it as input to the next sublayer, `which is the feedforward network`. The feedforward network **takes the attention matrix as input and returns the encoder representation as output.**\n",
    "4.  Next, we take the output obtained from encoder 1 and feed it as input to the encoder above it (encoder 2).\n",
    "5.  Encoder 2 carries the same process and returns the encoder representation of the given input sentence as output.\n",
    "\n",
    "6\n",
    "\n",
    "![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781838821593/files/assets/9129d96e-28a6-451d-9a47-a5eb88559cd1.png)\n",
    "\n",
    "> `the output (encoder representation) obtained from the final encoder (topmost encoder) will be the representation of the given input sentence`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Understanding the decoder of a transformer\n",
    "\n",
    "> A single decoder block with all of its components is shown in the following figure:\n",
    "\n",
    "![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781838821593/files/assets/7dfda3b3-c4fa-4428-90f2-77f8bc86f33a.png)\n",
    "\n",
    "#### Decoder have 3 sub layers\n",
    "\n",
    "-   Masked multi-head attention\n",
    "-   Multi-head attention\n",
    "-   Feedforward network\n",
    "\n",
    "#### Masked multi-head attention\n",
    "-  Decoder takes _<sos>_  as the first token, and combines `the next predicted word to the input on every time step` for predicting the target sentence until the _<eos>_  token is reached\n",
    "\n",
    "- To perform self-attention, we create three new matrices, called Q,k, V\n",
    "- Since we are computing multi-head attention, we create  `h` number of query, key, and value matrices.\n",
    "- Figure Masking the values\n",
    "\t- Masking words like this help the self-attention mechanism to attend only to the words that would be available to the model during testing.\t`\n",
    "- \n",
    "![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781838821593/files/assets/0c5b3654-a936-4d1b-95f3-0ae791ae57b7.png)\n",
    "\n",
    "> For example, look at the first row of our matrix. To predict the word next to the word _<sos>_, our model should not attend all the words to the right of _<sos>_  (as this will not be available during test time). So, we can mask all the words to the right of _<sos>_ with  -inf:\n",
    "\n",
    "![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781838821593/files/assets/c529bd7f-d3a0-44a2-97af-4ef32c97cd84.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "####  Multi-head attention\n",
    "- the multi-head attention sublayer in each decoder receives two inputs: \n",
    "\t- the previous sublayer, masked multi-head attention, a\n",
    "\t- encoder representation:\n",
    "![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781838821593/files/assets/07834a62-995a-48f7-bd8d-29db23d33910.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### R - encoder representation\n",
    "#### M -  matrix obtained as a result of the masked multi-head  attention  sublayer\n",
    "\n",
    "- We create the query matrix, Q , using the attention matrix,  `M -> Q` obtained from the previous sublayer.\n",
    "- create the key and value matrices using the encoder representation, `R->K, V`. \n",
    "![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781838821593/files/assets/e9c312cf-811f-45ba-a407-31be1f6acf74.png)\n",
    "\n",
    "###  how and why exactly is this useful?\n",
    "\n",
    "![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781838821593/files/assets/c8d934e3-bad8-43ed-bc7a-0f134cb6e6bf.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "By looking at the preceding matrix, , we can understand the following: \n",
    "\n",
    "- From the first row of the matrix, we can observe that we are computing the dot product between the query vector  (<sos>) and all the key vectors – `q1`(I), (am), and (good). Thus, the first row indicates how similar the target word <sos> is to all the words in the source sentence (I, am, and good).\n",
    "- Similarly, from the second row of the matrix, we can observe that we are computing the dot product between the query vector `q2` (Je) and all the key vectors –  (I), (am), and  (good). Thus, the second row indicates how similar the target word Je is to all the words in the source sentence (I, am, and good). \n",
    "- The same applies to all other rows. Thus, computing  helps us to understand how similar our query matrix (target sentence representation) is to the key matrix (source sentence representation). \n",
    "\n",
    "### ## Linear and softmax layer\n",
    "\n",
    "![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781838821593/files/assets/8aeae8dd-beed-436e-80db-14c5dda14410.png)\n",
    "\n",
    "- `The linear layer generates the logits` whose size is equal to our vocabulary size. \n",
    "- `convert the logits into a probability using the softmax function`, and then the decoder outputs the word whose index has a high probability value\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "From the preceding figure, we can understand the following:\n",
    "\n",
    "1.  First, we convert the input to the decoder into an embedding matrix and then add the position encoding to it and feed it as input to the bottom-most decoder (decoder 1).\n",
    "2.  The decoder takes the input and sends it to the masked multi-head attention layer, which returns the attention matrix, M, as output.\n",
    "3.  Now, we take the attention matrix, M, and also the encoder representation,  R, and feed them as input to the multi-head attention layer (encoder-decoder attention layer), which again outputs the new attention matrix.\n",
    "4.  We take the attention matrix obtained from the encoder-decoder attention layer and feed it as input to the next sublayer, which is the feedforward network. The feedforward network takes the attention matrix as input and returns the decoder representation as output.\n",
    "5.  Next, we take the output obtained from decoder 1 and feed it as input to the decoder above it (decoder 2).\n",
    "6.  Decoder 2 carries the same process and returns the decoder representation of the target sentence as output.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://towardsdatascience.com/different-normalization-layers-in-deep-learning-1a7214ff71d6\n",
    "- https://www.pinecone.io/learn/batch-layer-normalization/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
